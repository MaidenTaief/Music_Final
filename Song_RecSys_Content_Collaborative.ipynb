{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1L43HAZO0KhvViy-7_DPW0tuLV3N-eqRy",
      "authorship_tag": "ABX9TyOTjGmGJaHIaEh6wOKCW8jr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaidenTaief/Music_Final/blob/main/Song_RecSys_Content_Collaborative.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Collaborative Filtering\n",
        "\n",
        "###Loading and Preparing Data\n",
        "First, we set the path to our dataset and load our aggregated data, which includes user IDs and playlist names. This is crucial because we need to know which users interacted with which playlists. We then convert user IDs and playlist names into categorical data types. This conversion helps in indexing and speeds up operations since categorical data uses integers internally, which are faster to process than strings.\n",
        "\n",
        "To establish a direct mapping between user IDs and their indices in our matrices, as well as between playlist names and their indices, we create two dictionaries: `user_id_to_index` and `playlist_index_to_name`. These mappings are essential for efficiently looking up and referring to specific users and playlists in the subsequent steps.\n",
        "\n",
        "###Interaction Matrix and Similarity Calculation\n",
        "We load a pre-saved sparse matrix, `playlist_interaction_matrix`, that represents interactions between users and playlists. Using this matrix, we compute the cosine similarity among users. Cosine similarity helps us understand how similar two users are based on their interactions with various playlists. This similarity metric is a cornerstone of collaborative filtering, allowing us to predict a user's preferences based on the preferences of other similar users.\n",
        "\n",
        "###Recommendation Function\n",
        "\n",
        "The `recommend_playlists` function is designed to recommend playlist names to a user based on the preferences of similar users. Here's a breakdown of how this function works:\n",
        "\n",
        "- **User Index Conversion:** Convert the provided user ID to its corresponding index using the `user_id_to_index` mapping.\n",
        "- **Similarity Score Retrieval:** Fetch the cosine similarity scores for the given user against all other users.\n",
        "- **Score Aggregation:** Iterate through each user, weighted by their similarity to the target user, to compile a score for each playlist based on how those similar users interacted with the playlists.\n",
        "- **Result Compilation:** Zero out scores for playlists the user has already interacted with to ensure we only recommend new content. Then, sort the scores in descending order and pick the top N playlists.\n",
        "\n",
        "By focusing on playlists favored by users with similar tastes and excluding already familiar content, we can make personalized recommendations that are likely to be of interest.\n",
        "\n",
        "###Example Usage\n",
        "\n",
        "To demonstrate the functionality of this system, we use an example user ID and call the `recommend_playlists` function. This example illustrates how you can input any user ID from the dataset and retrieve personalized playlist recommendations.\n",
        "\n",
        "The output shows the top five recommended playlists for the user, providing a practical example of the recommendation system in action. These recommendations are tailored to the user's inferred preferences, showcasing the effectiveness of collaborative filtering."
      ],
      "metadata": {
        "id": "tVXpvi5lr6vh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "f7q5yhCy9KSC",
        "outputId": "c80d87d9-b5fe-41b1-d0cf-575773cc4a7e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e5f2162ddb88>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_npz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_IS_32BIT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_output\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_SetOutputMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m from .utils._tags import (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdeprecation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdiscovery\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mall_estimators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreadpool_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m from .validation import (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mthreadpoolctl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    606\u001b[0m from ._warnings_errors import (ConstantInputWarning, NearConstantInputWarning,\n\u001b[1;32m    607\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[0;32m--> 608\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_stats_py\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_variation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvariation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msuppress_warnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_measurements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m from scipy._lib._util import (check_random_state, MapWrapper, _get_nan,\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/spatial/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_plotutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_procrustes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprocrustes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_geometric_slerp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgeometric_slerp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;31m# Deprecated namespaces, to be removed in v2.0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/spatial/_geometric_slerp.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0meuclidean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mTYPE_CHECKING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_hausdorff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrel_entr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_distance_pybind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/special/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_spfun_stats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmultigammaln\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m from ._ellip_harm import (\n\u001b[0m\u001b[1;32m    785\u001b[0m     \u001b[0mellip_harm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0mellip_harm_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/special/_ellip_harm.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_ufuncs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_ellip_harm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_ellip_harm_2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_ellipsoid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ellipsoid_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from scipy.sparse import csr_matrix, load_npz\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Set the path to the dataset folder\n",
        "dataset_path = '/content/drive/My Drive/DATASET'\n",
        "\n",
        "# Load the aggregated data\n",
        "agg_data = pd.read_csv(f'{dataset_path}/aggregated_data.csv')\n",
        "\n",
        "# Convert user IDs and playlist names to categorical data types\n",
        "agg_data['user_id'] = pd.Categorical(agg_data['user_id'])\n",
        "agg_data['playlistname'] = pd.Categorical(agg_data['playlistname'])\n",
        "\n",
        "# Create mappings for user IDs and playlist names to their categorical indices\n",
        "user_id_to_index = {id: index for index, id in enumerate(agg_data['user_id'].cat.categories)}\n",
        "playlist_index_to_name = {index: name for index, name in enumerate(agg_data['playlistname'].cat.categories)}\n",
        "\n",
        "# Load the playlist interaction matrix\n",
        "playlist_interaction_matrix = load_npz(f'{dataset_path}/playlist_interaction_sparse_matrix.npz')\n",
        "\n",
        "# Compute the cosine similarity matrix for users\n",
        "user_similarity = cosine_similarity(playlist_interaction_matrix)\n",
        "\n",
        "# Function to recommend playlist names based on similar users' preferences\n",
        "def recommend_playlists(user_id, user_id_to_index, user_similarity_matrix, interaction_matrix, playlist_index_to_name, top_n=5):\n",
        "    \"\"\"\n",
        "    Recommend playlist names to a user based on what similar users liked.\n",
        "\n",
        "    Parameters:\n",
        "        user_id (str): User ID for whom recommendations are to be made.\n",
        "        user_id_to_index (dict): Dictionary mapping user IDs to their indices in the matrix.\n",
        "        user_similarity_matrix (numpy array): A 2D array of cosine similarities between users.\n",
        "        interaction_matrix (csr_matrix): Sparse matrix of user interactions with playlists.\n",
        "        playlist_index_to_name (dict): Dictionary mapping playlist indices to their names.\n",
        "        top_n (int): Number of playlists to recommend.\n",
        "\n",
        "    Returns:\n",
        "        list: Names of the top N recommended playlists for the user.\n",
        "    \"\"\"\n",
        "    user_index = user_id_to_index[user_id]  # Convert user_id to index\n",
        "    similarity_scores = user_similarity_matrix[user_index]\n",
        "    similar_users = np.argsort(-similarity_scores)\n",
        "    user_interactions = interaction_matrix[user_index].toarray()\n",
        "    playlist_scores = np.zeros(interaction_matrix.shape[1])\n",
        "\n",
        "    for other_user_index in similar_users:\n",
        "        if other_user_index == user_index:\n",
        "            continue\n",
        "        interaction_weight = similarity_scores[other_user_index]\n",
        "        other_user_interactions = interaction_matrix[other_user_index].toarray()\n",
        "        playlist_scores += other_user_interactions.flatten() * interaction_weight\n",
        "\n",
        "    playlist_scores[user_interactions.flatten() > 0] = 0\n",
        "    recommended_playlist_indices = np.argsort(-playlist_scores)[:top_n]\n",
        "    recommended_playlist_names = [playlist_index_to_name[idx] for idx in recommended_playlist_indices]\n",
        "\n",
        "    return recommended_playlist_names\n",
        "\n",
        "# Example use case\n",
        "example_user_id = '0007f3dd09c91198371454c608d47f22'\n",
        "recommended_playlists = recommend_playlists(example_user_id, user_id_to_index, user_similarity, playlist_interaction_matrix, playlist_index_to_name, top_n=5)\n",
        "print(f\"Recommended Playlists for User ID {example_user_id}:\", recommended_playlists)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Set the path to the dataset folder\n",
        "dataset_path = '/content/drive/My Drive/DATASET'\n",
        "\n",
        "# Load the aggregated data\n",
        "agg_data = pd.read_csv(f'{dataset_path}/aggregated_data.csv')\n",
        "\n",
        "# Get unique user IDs\n",
        "unique_user_ids = agg_data['user_id'].unique()\n",
        "\n",
        "# Print a few example user IDs\n",
        "print(\"Example User IDs:\", unique_user_ids[:5])  # Adjust the slice as needed to get more or fewer examples\n"
      ],
      "metadata": {
        "id": "TiUj2khm-e9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hybrid Filtering\n",
        "\n",
        "###Setting Up the Environment\n",
        "First off, we specify the location of our dataset and load it into our environment. This dataset includes both user IDs and playlist names, along with additional cluster information that we'll use later for content-based filtering. By converting user IDs and playlist names to categorical data types, we optimize the indexing process which is essential for handling large datasets efficiently.\n",
        "\n",
        "We also establish mappings for both user IDs and playlist names to numerical indices. These mappings are critical because they allow us to efficiently access and manipulate our data structures, especially when working with matrices.\n",
        "\n",
        "###Interaction Matrix and Similarity Computation\n",
        "\n",
        "Next, we load a pre-computed sparse matrix, `playlist_interaction_matrix`, that captures the interactions between users and playlists. This matrix is fundamental to calculating the cosine similarity among users, which we perform next. Cosine similarity measures how similar the interaction patterns of different users are, and it's a key component of collaborative filtering. This similarity helps us identify users with similar tastes and preferences.\n",
        "\n",
        "###Hybrid Recommendation Function\n",
        "\n",
        "The function `hybrid_recommend_playlists` is designed to leverage both collaborative and content-based filtering for recommending playlists:\n",
        "\n",
        "- **Collaborative Filtering**: We use the computed user similarity scores to identify users with similar playlist interaction patterns to the target user. By aggregating the interaction data from these similar users, we can predict which playlists might interest the target user.\n",
        "\n",
        "- **Content-Based Filtering**: We enhance our recommendations by considering the 'cluster' attribute of playlists. Playlists within the same cluster likely share certain characteristics or themes. By boosting the scores of playlists within the same cluster as those the user has shown interest in, we refine our recommendations to be more tailored and relevant.\n",
        "\n",
        "Each playlist's score is adjusted based on its cluster's relation to the user's preferences, ensuring that recommended playlists are not just popular among similar users but also contextually relevant.\n",
        "\n"
      ],
      "metadata": {
        "id": "6Wn7IEGLtBqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.sparse import csr_matrix, load_npz\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Set the path to the dataset folder\n",
        "dataset_path = '/content/drive/My Drive/DATASET'\n",
        "\n",
        "# Load the aggregated data, which includes cluster information\n",
        "agg_data = pd.read_csv(f'{dataset_path}/aggregated_data.csv')\n",
        "\n",
        "# Convert user IDs and playlist names to categorical types for indexing\n",
        "agg_data['user_id'] = pd.Categorical(agg_data['user_id'])\n",
        "agg_data['playlistname'] = pd.Categorical(agg_data['playlistname'])\n",
        "\n",
        "# Create mappings for user IDs and playlist names to their indices\n",
        "user_id_to_index = {id: index for index, id in enumerate(agg_data['user_id'].cat.categories)}\n",
        "playlist_index_to_name = {index: name for index, name in enumerate(agg_data['playlistname'].cat.categories)}\n",
        "\n",
        "# Load the interaction matrix for playlists\n",
        "playlist_interaction_matrix = load_npz(f'{dataset_path}/playlist_interaction_sparse_matrix.npz')\n",
        "\n",
        "# Compute cosine similarity among users based on their playlist interactions\n",
        "user_similarity = cosine_similarity(playlist_interaction_matrix)\n",
        "\n",
        "def hybrid_recommend_playlists(user_id, user_id_to_index, user_similarity_matrix, interaction_matrix, playlist_index_to_name, agg_data, top_n=5):\n",
        "    \"\"\"\n",
        "    Recommend playlists using a hybrid approach that combines collaborative filtering\n",
        "    with content-based filtering using playlist clusters.\n",
        "\n",
        "    Parameters:\n",
        "        user_id (str): User ID for whom recommendations are to be made.\n",
        "        user_id_to_index (dict): Dictionary mapping user IDs to their matrix indices.\n",
        "        user_similarity_matrix (numpy array): Array of cosine similarities between users.\n",
        "        interaction_matrix (csr_matrix): Matrix of user interactions with playlists.\n",
        "        playlist_index_to_name (dict): Dictionary mapping playlist indices to names.\n",
        "        agg_data (DataFrame): Aggregated data containing additional playlist attributes.\n",
        "        top_n (int): Number of playlists to recommend.\n",
        "\n",
        "    Returns:\n",
        "        list: Names of recommended playlists.\n",
        "    \"\"\"\n",
        "    user_index = user_id_to_index[user_id]\n",
        "    similarity_scores = user_similarity_matrix[user_index]\n",
        "    similar_users = np.argsort(-similarity_scores)\n",
        "    user_interactions = interaction_matrix[user_index].toarray()\n",
        "    playlist_scores = np.zeros(interaction_matrix.shape[1])\n",
        "\n",
        "    for other_user_index in similar_users:\n",
        "        if other_user_index == user_index:\n",
        "            continue\n",
        "        interaction_weight = similarity_scores[other_user_index]\n",
        "        other_user_interactions = interaction_matrix[other_user_index].toarray()\n",
        "        playlist_scores += other_user_interactions.flatten() * interaction_weight\n",
        "\n",
        "    playlist_scores[user_interactions.flatten() > 0] = 0\n",
        "\n",
        "    # Adjust scores based on content (clusters)\n",
        "    for playlist_index in np.where(playlist_scores > 0)[0]:\n",
        "        playlist_name = playlist_index_to_name[playlist_index]\n",
        "        cluster_id = agg_data.loc[agg_data['playlistname'] == playlist_name, 'cluster'].iloc[0]\n",
        "        # Increase score slightly for playlists in the same cluster\n",
        "        playlist_scores[playlist_index] *= 1 + 0.1 * cluster_id\n",
        "\n",
        "    recommended_playlist_indices = np.argsort(-playlist_scores)[:top_n]\n",
        "    recommended_playlist_names = [playlist_index_to_name[idx] for idx in recommended_playlist_indices]\n",
        "\n",
        "    return recommended_playlist_names\n",
        "\n",
        "example_user_id = '0007f3dd09c91198371454c608d47f22'\n",
        "recommended_playlists = hybrid_recommend_playlists(example_user_id, user_id_to_index, user_similarity, playlist_interaction_matrix, playlist_index_to_name, agg_data, top_n=5)\n",
        "print(f\"Recommended Playlists for User ID {example_user_id}:\", recommended_playlists)"
      ],
      "metadata": {
        "id": "3AkmNMTvZNMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Setup and Data Preparation\n",
        "We start by setting the location of our dataset and loading the song data that includes important cluster information. This data is crucial as it contains the interactions between users and songs, which are fundamental to understanding user preferences.\n",
        "\n",
        "To efficiently handle and reference the song data, we convert song names into categorical data types. This not only optimizes our operations but also allows us to effectively index songs, which is essential for the recommendation process.\n",
        "\n",
        "By creating mappings from song names to indices, we ensure quick and direct access to song data throughout our recommendation computations, enhancing the systemâ€™s efficiency.\n",
        "\n",
        "###Dimensionality Reduction with Truncated SVD\n",
        "To manage the potentially large and sparse song interaction matrix, we apply Truncated SVD (Singular Value Decomposition). This method reduces the dimensionality of our dataset while preserving its significant characteristics. Here, we choose 200 components, a number that balances complexity and performance, making our dataset more manageable and speeding up subsequent calculations.\n",
        "\n",
        "Reducing dimensions helps in mitigating issues like overfitting and computational overload, especially when dealing with large datasets. It also aids in extracting the latent factors that represent underlying patterns in song interactions.\n",
        "\n",
        "###Our recommendation function, `Hybrid_recommend_songs`, combines collaborative filtering and content-based filtering techniques:\n",
        "\n",
        "- **Collaborative Filtering**: We calculate the cosine similarity between the target song vector (from our reduced matrix) and all other song vectors. This metric helps us find songs that share similar interaction patterns, suggesting that users who liked one may like the others.\n",
        "\n",
        "- **Content-Based Filtering**: We enhance our recommendations using the cluster information from the songs. Songs within the same cluster typically share thematic or stylistic similarities. We give a higher weight to songs from the same cluster as the target song, refining our recommendations to be contextually relevant.\n",
        "\n",
        "The function fetches additional candidates to ensure robust filtering and then ranks these songs based on their computed similarity scores, adjusted for content relevance.\n",
        "\n"
      ],
      "metadata": {
        "id": "T3eQMIbFuOpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Set the path to the dataset folder\n",
        "dataset_path = '/content/drive/My Drive/DATASET'\n",
        "\n",
        "# Load the aggregated data\n",
        "agg_data = pd.read_csv(f'{dataset_path}/aggregated_data.csv')\n",
        "\n",
        "# Get unique user IDs\n",
        "unique_song_name = agg_data['name'].unique()\n",
        "\n",
        "# Print a few example user IDs\n",
        "print(\"Example song names:\", unique_song_name[:5])  # Adjust the slice as needed to get more or fewer examples\n"
      ],
      "metadata": {
        "id": "MTFohmNObZRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.sparse import csr_matrix, load_npz\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import numpy as np\n",
        "\n",
        "# Set the path to the dataset folder\n",
        "dataset_path = '/content/drive/My Drive/DATASET'\n",
        "\n",
        "# Load the aggregated song data with cluster information\n",
        "song_data = pd.read_csv(f'{dataset_path}/aggregated_data.csv')\n",
        "\n",
        "# Convert song names to categorical for indexing\n",
        "song_data['name'] = pd.Categorical(song_data['name'])\n",
        "\n",
        "# Create mappings for song names to indices\n",
        "song_name_to_index = {name: index for index, name in enumerate(song_data['name'].cat.categories)}\n",
        "\n",
        "# Load the interaction matrix for songs\n",
        "song_interaction_matrix = load_npz(f'{dataset_path}/sparse_song_matrix.npz')\n",
        "\n",
        "# Applying Truncated SVD to reduce dimensions\n",
        "svd = TruncatedSVD(n_components=200)  # Adjust n_components based on the dataset size and available memory\n",
        "reduced_matrix = svd.fit_transform(song_interaction_matrix.transpose())\n",
        "\n",
        "def Hybrid_recommend_songs(song_name, song_name_to_index, reduced_matrix, song_data, top_n=5):\n",
        "    song_index = song_name_to_index[song_name]\n",
        "    target_song_vector = reduced_matrix[song_index]\n",
        "\n",
        "    # Calculate cosine similarity between the target song vector and all other song vectors\n",
        "    similarity_scores = cosine_similarity([target_song_vector], reduced_matrix)[0]\n",
        "\n",
        "    similar_songs_indices = np.argsort(-similarity_scores)[1:top_n+10]  # Fetch more to filter by cluster\n",
        "\n",
        "    # Use content-based filtering to refine recommendations\n",
        "    song_cluster = song_data.loc[song_data['name'] == song_name, 'cluster'].iloc[0]\n",
        "    enhanced_scores = []\n",
        "    for idx in similar_songs_indices:\n",
        "        song_name = song_data['name'].cat.categories[idx]\n",
        "        current_song_cluster = song_data.loc[song_data['name'] == song_name, 'cluster'].iloc[0]\n",
        "        if song_cluster == current_song_cluster:\n",
        "            enhanced_scores.append((similarity_scores[idx] * 1.1, idx))  # Boost score for same-cluster songs\n",
        "        else:\n",
        "            enhanced_scores.append((similarity_scores[idx], idx))\n",
        "\n",
        "    # Sort by enhanced score and select top N\n",
        "    recommended_songs = sorted(enhanced_scores, reverse=True, key=lambda x: x[0])[:top_n]\n",
        "    recommended_song_names = [song_data['name'].cat.categories[idx] for _, idx in recommended_songs]\n",
        "\n",
        "    return recommended_song_names\n",
        "\n",
        "input_song_name = 'November Rain'  # Replace with actual song name\n",
        "recommended_songs = Hybrid_recommend_songs(input_song_name, song_name_to_index, reduced_matrix, song_data, top_n=5)\n",
        "print(f\"Recommended Songs for {input_song_name}:\", recommended_songs)"
      ],
      "metadata": {
        "id": "tMxVODLtbE8_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}